defaults:
  - dataset: tweet
  - model/encoder: lstm
  - model/decoder: perceptron
  - loss: [cce]

data_loader:
  train:
    num_workers: 16
    persistent_workers: true
    batch_size: 4
    random_seed: 42
  test:
    num_workers: 4
    persistent_workers: false
    batch_size: 1
    random_seed: 42
  val:
    num_workers: 1
    persistent_workers: true
    batch_size: 1
    random_seed: 42

trainer:
  total_iter: 100000
  optimizer:
    gradient_clip_val: 0.01
    warm_up_steps: 2000
    lr: 1.5e-4
    betas:
      - 0.9
      - 0.999
  checkpointing:
    pretrain_ckpt: null
    checkpoint_iter: 5000
  evaluation:
    validate_iter: 5000
    test_iter: 10000