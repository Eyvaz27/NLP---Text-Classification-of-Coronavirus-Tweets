defaults:
  - dataset: tweet
  - model/encoder: lstm
  - model/decoder: perceptron
  - loss: [cce]

data_loader:
  train:
    num_workers: 16
    persistent_workers: true
    batch_size: 4
    seed: 42
  test:
    num_workers: 4
    persistent_workers: false
    batch_size: 1
    seed: 42
  val:
    num_workers: 1
    persistent_workers: true
    batch_size: 1
    seed: 42

optimizer:
  warm_up_steps: 2000
  lr: 1.5e-4
  betas:
    - 0.9
    - 0.999

checkpointing:
  pretrain_ckpt: null
  every_n_train_steps: 5000
